{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Importación de librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importa las librerías necesarias:\n",
    "\n",
    "* tensorflow: Principal librería usada para construir y entrenar la CNN.\n",
    "* Path de pathlib: Sirve para manejar rutas de archivos y directorios de manera flexible.\n",
    "* Sequential, Conv2D, y MaxPooling2D de tensorflow.keras.layers: Componentes específicos para construir una red CNN secuencial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Obtención del directorio de trabajo actual**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karlo\\Desktop\\Emotion_recognition\\SW_Inteligente_Tarea_4\n"
     ]
    }
   ],
   "source": [
    "current_directory = Path(os.getcwd())\n",
    "print(current_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtiene el directorio de trabajo actual y lo guarda en current_directory. La función print imprime este directorio para confirmar que el script está corriendo en la ruta correcta, lo cual es útil para organizar rutas de archivos en sistemas operativos diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Generador de datos de entrenamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "training_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "training_gen = training_data_gen.flow_from_directory(directory=f'{current_directory}/data/train', target_size=(48, 48), color_mode='grayscale', batch_size=32, class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un generador de datos para el conjunto de entrenamiento:\n",
    "\n",
    "* ImageDataGenerator(rescale=1./255): Normaliza los valores de los píxeles de las imágenes entre 0 y 1 (originalmente están entre 0 y 255).\n",
    "* flow_from_directory: Carga las imágenes desde el directorio /data/train, con las siguientes configuraciones:\n",
    "    * target_size=(48, 48): Redimensiona las imágenes a 48x48 píxeles.\n",
    "    * color_mode='grayscale': Carga las imágenes en escala de grises (1 canal).\n",
    "    * batch_size=32: Genera los datos en lotes de 32 imágenes.\n",
    "    * class_mode='categorical': Usa la clasificación categórica para múltiples categorías de emociones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Generador de datos de prueba**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "test_gen = test_data_gen.flow_from_directory(directory=f'{current_directory}/data/test', target_size=(48, 48), color_mode='grayscale', batch_size=32, class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar al anterior, pero se configura para el conjunto de prueba en /data/test. Permite evaluar el modelo en datos no vistos, garantizando que el modelo generaliza bien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Definición del modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una instancia Sequential, una arquitectura lineal de capas para la CNN. También se define una lista emotions que contiene las categorías de emociones (etiquetas de salida), utilizada más adelante para determinar el tamaño de la capa de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Capas convolucionales y de agrupamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karlo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se agrega capas de convolución y agrupamiento:\n",
    "\n",
    "* Primera capa: Conv2D con 32 filtros y tamaño de kernel 3x3, relu como función de activación y input_shape=(48, 48, 1) (48x48 píxeles y 1 canal).\n",
    "* Primera capa MaxPooling: Reduce la dimensión de la imagen a la mitad usando una ventana de 2x2.\n",
    "* Segunda capa Conv2D: Conv2D con 64 filtros y un tamaño de kernel de 3x3, seguido de otra capa MaxPooling.\n",
    "* Tercera capa Conv2D: Nueva capa Conv2D de 64 filtros.\n",
    "\n",
    "Estas capas ayudan a extraer características de las imágenes, como bordes y texturas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. Aplanamiento de las salidas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convierte la salida tridimensional de las capas convolucionales en una sola dimensión, preparándola para las capas densas que seguirán."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8. Capas densas para clasificación**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(len(emotions), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se agrega capas densas para la clasificación:\n",
    "\n",
    "* La primera capa densa tiene 64 unidades y relu como función de activación.\n",
    "* La segunda capa densa (capa de salida) tiene tantas unidades como categorías de emociones (len(emotions)), con activación softmax para producir una distribución de probabilidad para cada clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **9. Compilación del modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se compila el modelo con:\n",
    "\n",
    "* optimizer='adam': Optimizador Adam para ajustar los pesos del modelo.\n",
    "* loss='categorical_crossentropy': Pérdida adecuada para clasificación multiclase.\n",
    "* metrics=['accuracy']: Métrica de precisión para monitorear el rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **10. Entrenamiento del modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - accuracy: 0.4682 - loss: 1.3877 - val_accuracy: 0.4702 - val_loss: 1.3883\n",
      "Epoch 2/20\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - accuracy: 0.5244 - loss: 1.2540 - val_accuracy: 0.5244 - val_loss: 1.2511\n",
      "Epoch 3/20\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 19ms/step - accuracy: 0.5719 - loss: 1.1417 - val_accuracy: 0.5293 - val_loss: 1.2327\n",
      "Epoch 4/20\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - accuracy: 0.6003 - loss: 1.0661 - val_accuracy: 0.5435 - val_loss: 1.2125\n",
      "Epoch 5/20\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - accuracy: 0.6333 - loss: 0.9841 - val_accuracy: 0.5578 - val_loss: 1.1955\n",
      "Epoch 6/20\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - accuracy: 0.6687 - loss: 0.9002 - val_accuracy: 0.5396 - val_loss: 1.2579\n",
      "Epoch 7/20\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - accuracy: 0.6964 - loss: 0.8222 - val_accuracy: 0.5414 - val_loss: 1.2979\n",
      "Epoch 8/20\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - accuracy: 0.7311 - loss: 0.7417 - val_accuracy: 0.5440 - val_loss: 1.3381\n",
      "Epoch 9/20\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 21ms/step - accuracy: 0.7626 - loss: 0.6625 - val_accuracy: 0.5435 - val_loss: 1.4083\n",
      "Epoch 10/20\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 21ms/step - accuracy: 0.7914 - loss: 0.5881 - val_accuracy: 0.5471 - val_loss: 1.4722\n",
      "Epoch 11/20\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - accuracy: 0.8194 - loss: 0.5181 - val_accuracy: 0.5513 - val_loss: 1.6217\n",
      "Epoch 12/20\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - accuracy: 0.8438 - loss: 0.4420 - val_accuracy: 0.5449 - val_loss: 1.6965\n",
      "Epoch 13/20\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 21ms/step - accuracy: 0.8619 - loss: 0.3894 - val_accuracy: 0.5369 - val_loss: 1.9079\n",
      "Epoch 14/20\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - accuracy: 0.8877 - loss: 0.3284 - val_accuracy: 0.5351 - val_loss: 2.0436\n",
      "Epoch 15/20\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - accuracy: 0.9045 - loss: 0.2847 - val_accuracy: 0.5421 - val_loss: 2.2331\n",
      "Epoch 16/20\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - accuracy: 0.9174 - loss: 0.2511 - val_accuracy: 0.5234 - val_loss: 2.4224\n",
      "Epoch 17/20\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - accuracy: 0.9276 - loss: 0.2193 - val_accuracy: 0.5233 - val_loss: 2.5988\n",
      "Epoch 18/20\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 21ms/step - accuracy: 0.9332 - loss: 0.1973 - val_accuracy: 0.5391 - val_loss: 2.7958\n",
      "Epoch 19/20\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 21ms/step - accuracy: 0.9447 - loss: 0.1676 - val_accuracy: 0.5259 - val_loss: 2.8627\n",
      "Epoch 20/20\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - accuracy: 0.9505 - loss: 0.1487 - val_accuracy: 0.5295 - val_loss: 3.1595\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(training_gen, epochs=20, validation_data=test_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este bloque entrena el modelo:\n",
    "\n",
    "* training_gen: Datos de entrenamiento.\n",
    "* epochs=20: Número de épocas para entrenar.\n",
    "* validation_data=test_gen: Datos de validación para evaluar el modelo al final de cada época."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **11. Guardado del modelo entrenado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(f'{current_directory}/model/trained_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, el modelo entrenado se guarda en un archivo .h5 en el directorio /model/. Esto permite reutilizar el modelo sin necesidad de volver a entrenarlo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
